{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "from _paths import nomeroff_net_dir\n",
    "\n",
    "from nomeroff_net.pipes.number_plate_text_readers.base.ocr import OCR\n",
    "from nomeroff_net.tools.mcm import modelhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto download latest dataset\n",
    "from nomeroff_net.tools import modelhub\n",
    "\n",
    "# auto download latest dataset\n",
    "#info = modelhub.download_dataset_for_model(\"EuUa1995\")\n",
    "#PATH_TO_DATASET = info[\"dataset_path\"]\n",
    "PATH_TO_DATASET = os.path.join(nomeroff_net_dir, \"data/dataset/TextDetector/ocr_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(model_path='latest',\n",
    "           text_detector_name = \"eu_ua_2004_2015\",\n",
    "           img_format = \"png\",\n",
    "           root_dir=os.path.join(PATH_TO_DATASET, \"test\"),\n",
    "           predicted_part_size=1,\n",
    "           acc_less_than = 0.7,\n",
    "           replace_tamplate = None,\n",
    "           model_conf = None):\n",
    "    if replace_tamplate is None:\n",
    "        replace_tamplate = {'moderation': {'isModerated': 1, 'moderatedBy': 'ApelSYN'}}\n",
    "    if model_conf is None:\n",
    "        model_conf = copy.deepcopy(modelhub.models[text_detector_name])\n",
    "    text_detector = OCR(model_name=text_detector_name, letters=model_conf[\"letters\"],\n",
    "                        linear_size=model_conf[\"linear_size\"], max_text_len=model_conf[\"max_text_len\"],\n",
    "                        height=model_conf[\"height\"], width=model_conf[\"width\"],\n",
    "                        color_channels=model_conf[\"color_channels\"],\n",
    "                        hidden_size=model_conf[\"hidden_size\"], backbone=model_conf[\"backbone\"])\n",
    "    text_detector.init_label_converter()\n",
    "    text_detector.load(model_path)\n",
    "\n",
    "    ann_dir = os.path.join(root_dir, \"ann\")\n",
    "    jsons = []\n",
    "    jsons_paths = []\n",
    "    for dir_name, subdir_list, file_list in os.walk(ann_dir):\n",
    "        for fname in file_list:\n",
    "            fname = os.path.join(ann_dir, fname)\n",
    "            jsons_paths.append(fname)\n",
    "            with open(fname) as jsonF:\n",
    "                jsonData = json.load(jsonF)\n",
    "            jsons.append(jsonData)\n",
    "    print(\"LOADED {} ANNOTATIONS\".format(len(jsons)))\n",
    "\n",
    "    img_dir = os.path.join(root_dir, \"img\")\n",
    "    imgs = []\n",
    "    for j in jsons:\n",
    "        img_path =os.path.join(img_dir, \"{}.{}\".format(j[\"name\"], img_format))\n",
    "        img = cv2.imread(img_path)\n",
    "        imgs.append(img)\n",
    "    print(\"LOADED {} IMAGES\".format(len(imgs)))\n",
    "\n",
    "    predicted = []\n",
    "    accs      = []\n",
    "    N = math.ceil(len(imgs) / predicted_part_size)\n",
    "    for i in range(N):\n",
    "        part = i*predicted_part_size\n",
    "        part_imgs = imgs[part:part+predicted_part_size]\n",
    "        xs = text_detector.preprocess(part_imgs)\n",
    "        predicted_part, net_out_value_part = text_detector.predict(xs, return_acc=True)\n",
    "        predicted += predicted_part\n",
    "\n",
    "\n",
    "        # get accuracy\n",
    "        if acc_less_than >= 1:\n",
    "            # not process acc\n",
    "            accs  += [1 for _predicted in predicted_part]\n",
    "            continue\n",
    "        # process accuracy\n",
    "        acc_part = []\n",
    "        for _predicted, _net_out_value in zip(predicted_part, net_out_value_part):\n",
    "            acc = text_detector.get_acc([_net_out_value], [_predicted])\n",
    "            acc_part.append(acc)\n",
    "        accs  += acc_part\n",
    "\n",
    "\n",
    "    print(\"PREDICTED {} IMAGES\".format(len(predicted)))\n",
    "\n",
    "    err_cnt = 0\n",
    "    for i in range(len(jsons_paths)):\n",
    "        json_path      = jsons_paths[i]\n",
    "        predicted_item = predicted[i]\n",
    "        jsonData       = jsons[i]\n",
    "        acc            = accs[i]\n",
    "        jsonData[\"moderation\"][\"predicted\"] = predicted_item\n",
    "\n",
    "        if jsonData[\"description\"].lower() == jsonData[\"moderation\"][\"predicted\"].lower() and acc > acc_less_than:\n",
    "            jsonData[\"moderation\"][\"isModerated\"] = 1\n",
    "        else:\n",
    "            print(\"Predicted '{}' with acc {}, real: '{}' in file {}\".format(\n",
    "                jsonData[\"moderation\"][\"predicted\"].lower(),\n",
    "                acc,\n",
    "                jsonData[\"description\"].lower(),\n",
    "                json_path))\n",
    "            err_cnt = err_cnt+1\n",
    "            jsonData[\"moderation\"][\"isModerated\"] = 0\n",
    "        with open(json_path, \"w\", encoding='utf8') as jsonWF:\n",
    "            json.dump(jsonData, jsonWF,  ensure_ascii=False)\n",
    "\n",
    "    print(\"Error detection count: {}\".format(err_cnt))\n",
    "    print(\"Accuracy: {}\".format(1-err_cnt/len(predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED 4 ANNOTATIONS\n",
      "LOADED 4 IMAGES\n",
      "PREDICTED 4 IMAGES\n",
      "Predicted '379895b' with acc 0.9744253754615784, real: '0038sc' in file /mnt/data/var/www/nomeroff-net/examples/ju/dataset_tools/../../../data/dataset/TextDetector/ocr_example/test/ann/0038SC-0.json\n",
      "Predicted '371kpk' with acc 0.8737720251083374, real: 'x371hk96' in file /mnt/data/var/www/nomeroff-net/examples/ju/dataset_tools/../../../data/dataset/TextDetector/ocr_example/test/ann/1000_X371HK96_0.json\n",
      "Predicted '13bm' with acc 0.6171997785568237, real: 'aa0013bm' in file /mnt/data/var/www/nomeroff-net/examples/ju/dataset_tools/../../../data/dataset/TextDetector/ocr_example/test/ann/AA0013BM.json\n",
      "Predicted '20146kc' with acc 0.9199544191360474, real: '2914 kc-7' in file /mnt/data/var/www/nomeroff-net/examples/ju/dataset_tools/../../../data/dataset/TextDetector/ocr_example/test/ann/10001_2914KC7_0.json\n",
      "Error detection count: 4\n",
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "compare(model_path=\"latest\", acc_less_than = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED 4 ANNOTATIONS\n",
      "LOADED 4 IMAGES\n",
      "PREDICTED 4 IMAGES\n",
      "Predicted 'bp93bp' with acc 0.7681334018707275, real: '0038sc' in file /mnt/data/var/www/nomeroff-net/examples/ju/dataset_tools/../../../data/dataset/TextDetector/ocr_example/test/ann/0038SC-0.json\n",
      "Predicted 'bh71p' with acc 0.8713468909263611, real: 'x371hk96' in file /mnt/data/var/www/nomeroff-net/examples/ju/dataset_tools/../../../data/dataset/TextDetector/ocr_example/test/ann/1000_X371HK96_0.json\n",
      "Predicted 'a2916ct' with acc 0.9012768268585205, real: '2914 kc-7' in file /mnt/data/var/www/nomeroff-net/examples/ju/dataset_tools/../../../data/dataset/TextDetector/ocr_example/test/ann/10001_2914KC7_0.json\n",
      "Error detection count: 3\n",
      "Accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "# # or load your own trained model\n",
    "# compare(\n",
    "#     model_path=\"../../../data/models/shufflenet_v2_x2_0/anpr_ocr_eu_2004_2015_2022_11_11_shufflenet_v2_x2_0.ckpt\",\n",
    "#     acc_less_than = 0.75,\n",
    "#     model_conf={\n",
    "#       \"backbone\": \"shufflenet_v2_x2_0\",\n",
    "#       \"letters\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\",\n",
    "#                   \"I\", \"K\", \"M\", \"O\", \"P\", \"T\", \"X\", \"Y\", \"Z\"],\n",
    "#       \"max_text_len\": 8,\n",
    "#       \"height\": 50,\n",
    "#       \"width\": 200,\n",
    "#       \"color_channels\": 3,\n",
    "#       \"bidirectional\": True,\n",
    "#       \"hidden_size\": 32,\n",
    "#       \"linear_size\": 416\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
